{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imports are necessary for all three stages below, but each stage should be able to be run given that the previous stage has been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "import datetime\n",
    "\n",
    "from youtube_dl import YoutubeDL\n",
    "\n",
    "from movie_data_scraper import TMDB_Scraper\n",
    "\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from apiclient.discovery import build\n",
    "from apiclient.http import MediaFileUpload\n",
    "from apiclient import errors\n",
    "\n",
    "yt_url_prefix = 'https://www.youtube.com/watch?v='\n",
    "output_folder = 'data/'\n",
    "tmdb_scraper = TMDB_Scraper(api_key=\"abd17a9f250807b76ebbfa9997ca6ade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Find Movie Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a preliminary search of the results we want to aggregate using tMDb discover API, storing the total number of pages so we can aggregate all of them by iterating later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discover_query_results = tmdb_scraper.run_api(\"discover/movie\", js_query_args={\n",
    "    'sort_by': 'vote_average.desc',\n",
    "    'vote_count.gte': 2000\n",
    "})\n",
    "num_pages = discover_query_results['total_pages']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through and aggregate all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_movie_ids(cache_file=None):\n",
    "    \"\"\"Get the TMDb trailer IDs of the movies we want to analyze. Currently, the default setting collects\n",
    "    highest rated movies. The optional cache_file argument will allow the user to, instead of collecting the\n",
    "    most up-to-date information, use a cached and dated version.\"\"\"\n",
    "    if cache_file:\n",
    "        return np.load(get_tmdb_cache_path(cache_file))\n",
    "    else:\n",
    "        trailer_ids = []\n",
    "        for p in range(1, num_pages+1):\n",
    "            this_page_results = tmdb_scraper.run_api(\"discover/movie\", js_query_args={\n",
    "                'sort_by': 'vote_average.desc',\n",
    "                'vote_count.gte': 2000,\n",
    "                'page': p\n",
    "            })\n",
    "            print(f\"Aggregating movie IDs from page {p} of {num_pages}\", end='\\r')\n",
    "            trailer_ids.extend(m['id'] for m in this_page_results['results'])\n",
    "        return np.array(trailer_ids)\n",
    "        \n",
    "def get_tmdb_cache_path(cache_file):\n",
    "    return output_folder + 'tmdb_id_history/' + cache_file + \".npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   278,    238,    424, ..., 166424,    415, 351460])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmdb_ids = get_movie_ids('2019-10-30_01-28-00')\n",
    "#tmdb_ids = get_trailer_ids() # Use this line instead to get new, current data for new analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save these TMDb id's with current datetime.\n",
    "# np.save(get_tmdb_cache_path(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")), tmdb_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Aggregate Movie Metadata, Crew Metadata, and Trailer Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect Data in-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating data from of movie 1186 of 1187\r"
     ]
    }
   ],
   "source": [
    "# Make Blank DataFrame to append results to\n",
    "all_trailers = pd.DataFrame({\"tmdb_id\": [], 'tmdb_title': [], \"trailer_title\": [], \"trailer_youtube_key\": []})\n",
    "# Make empty json's to append results to\n",
    "movie_details = {}\n",
    "movie_crew = {}\n",
    "\n",
    "for i, tmdb_id in enumerate(tmdb_ids):\n",
    "    print(f\"Aggregating data from of movie {i} of {len(tmdb_ids)}\", end='\\r')\n",
    "    this_movie_details = tmdb_scraper.movie_details(tmdb_id)\n",
    "    movie_details[str(tmdb_id)] = this_movie_details\n",
    "    movie_crew[str(tmdb_id)] = tmdb_scraper.movie_crew(tmdb_id)\n",
    "    videos = tmdb_scraper.movie_videos(tmdb_id)\n",
    "    trailers = [v for v in videos['results'] if v['type'] == 'Trailer']\n",
    "    for t in trailers:\n",
    "        all_trailers = all_trailers.append({\n",
    "            \"tmdb_id\": this_movie_details['id'],\n",
    "            'tmdb_title': this_movie_details['title'],\n",
    "            'trailer_title': t['name'],\n",
    "            'trailer_youtube_key': t['key']\n",
    "        }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_trailers.to_csv(osp.join(output_folder, \"trailers.csv\"))\n",
    "with open(osp.join(output_folder, 'movie_details.json'), 'w') as outfile:\n",
    "    json.dump(movie_details, outfile)\n",
    "\n",
    "with open(osp.join(output_folder, 'movie_crew.json'), 'w') as outfile:\n",
    "    json.dump(movie_crew, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Upload Movie Trailers to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies\n",
    "#!pip install pandas\n",
    "#!pip install youtube_dl\n",
    "#!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GOOGLE DRIVE API CONSTANTS\n",
    "flow = InstalledAppFlow.from_client_secrets_file('credentials.json', ['https://www.googleapis.com/auth/drive'])\n",
    "creds = flow.run_local_server(port=0)\n",
    "folder_id = '1UnDIe4VHMM8bZzIfKEG8NJdQGnNrrxEG'\n",
    "drive_api = build('drive', 'v3', credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helpful function\n",
    "def get_file_list_from_folder(service, folder_id):\n",
    "    \"\"\"Print files belonging to a folder.\n",
    "\n",
    "    Args:\n",
    "    service: Drive API service instance.\n",
    "    folder_id: ID of the folder to print files from.\n",
    "    \"\"\"\n",
    "    kwargs = {\n",
    "        \"q\": \"'{}' in parents\".format(folder_id)\n",
    "    }\n",
    "    request = service.files().list(**kwargs)\n",
    "    files = []\n",
    "    while request is not None:\n",
    "        response = request.execute()\n",
    "        # Do stuff with response['files']\n",
    "        files.extend(response['files'])\n",
    "        request = service.files().list_next(request, response)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1171"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trailers = pd.read_csv(osp.join(output_folder, \"trailers.csv\"), index_col=0)\n",
    "trailers['movie_title'].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ydl_opts = {\n",
    "    'outtmpl': output_folder + '%(id)s.%(ext)s',\n",
    "    'format': 'bestvideo'\n",
    "}\n",
    "\n",
    "uploaded = get_file_list_from_folder(drive_api, folder_id)\n",
    "#print(uploaded)\n",
    "uploaded = [d['name'].split('.')[0] for d in uploaded]\n",
    "duplicates = pd.Series(uploaded).value_counts()\n",
    "uploaded = set(uploaded)\n",
    "#print(duplicates)\n",
    "#print(f'files already uploaded {uploaded}')\n",
    "\n",
    "all_to_download = set(trailers['youtube_key'])\n",
    "#print(f'files already uploaded & requested for upload{uploaded & all_to_download}')\n",
    "remaining_trailers = all_to_download - uploaded\n",
    "print(f'remaining trailers (len({len(remaining_trailers)})) {remaining_trailers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with YoutubeDL(ydl_opts) as ydl:\n",
    "    for t in remaining_trailers:\n",
    "        try:\n",
    "            # Helpful Constants\n",
    "            yt_link = yt_url_prefix + t\n",
    "\n",
    "            # Download Video\n",
    "            print(f\"Downloading {t}\")\n",
    "            trailer_yt_info = ydl.extract_info(yt_link, download=True)\n",
    "            \n",
    "            # Determine File Name\n",
    "            file_name = [f for f in os.listdir(output_folder) if t in f][0]\n",
    "            print(file_name)\n",
    "\n",
    "            # Upload to Drive\n",
    "            print(f\"Uploading file {file_name}...\")\n",
    "            body = {'name': file_name, 'parents': [folder_id]}\n",
    "            media = MediaFileUpload(output_folder + file_name)\n",
    "            fiahl = drive_api.files().create(body=body, media_body=media).execute()\n",
    "            print(f\"Created file '{fiahl.get('name')}' id '{fiahl.get('id')}'.\")\n",
    "\n",
    "            # Delete video from hard drive\n",
    "            os.remove(output_folder + file_name)\n",
    "            print(f\"Removed {file_name}\")\n",
    "            print(\"Success!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed with exception\")\n",
    "            print(e)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
